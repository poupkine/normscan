# train_model.py
from backend.app.models.autoencoder_2d import Autoencoder2D
from backend.app.services.dicom_loader import load_slices_from_zip
import os
import sys
import json
import time
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from pathlib import Path
import logging

# –î–æ–±–∞–≤–ª—è–µ–º backend –≤ –ø—É—Ç—å, —á—Ç–æ–±—ã –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥—É–ª–∏
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root / "backend"))


# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# –ü—É—Ç–∏
DATA_DIR = project_root / "backend" / "data" / "train"
MODEL_SAVE_PATH = project_root / "backend" / "app" / "static" / "model_weights" / "autoencoder_2d.pth"
METRICS_REPORT_PATH = project_root / "metrics_report.json"
VAL_REPORT_PATH = project_root / "validation_report.xlsx"

# –°–æ–∑–¥–∞—ë–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
MODEL_SAVE_PATH.parent.mkdir(parents=True, exist_ok=True)


def load_all_slices_from_normal(data_dir: Path, max_files: int = None) -> np.ndarray:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ —Å—Ä–µ–∑—ã –∏–∑ ZIP-—Ñ–∞–π–ª–æ–≤ –≤ –ø–∞–ø–∫–µ normal/"""
    normal_dir = data_dir / "normal"
    if not normal_dir.exists():
        raise FileNotFoundError(f"–ü–∞–ø–∫–∞ {normal_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –ª–µ–∂–∞—Ç –≤ backend/data/train/normal/")

    all_slices = []
    zip_files = list(normal_dir.glob("*.zip"))
    if max_files:
        zip_files = zip_files[:max_files]

    logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(zip_files)} ZIP-—Ñ–∞–π–ª–æ–≤ –≤ {normal_dir}")
    for i, zip_path in enumerate(zip_files):
        logger.info(f"[{i + 1}/{len(zip_files)}] –ó–∞–≥—Ä—É–∑–∫–∞ {zip_path.name}...")
        try:
            slices, _, _ = load_slices_from_zip(str(zip_path))
            all_slices.append(slices)
        except Exception as e:
            logger.warning(f"–ü—Ä–æ–ø—É—â–µ–Ω —Ñ–∞–π–ª {zip_path}: {e}")

    if not all_slices:
        raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ —Å—Ä–µ–∑–∞!")

    all_slices = np.concatenate(all_slices, axis=0)
    logger.info(f"–í—Å–µ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(all_slices)} —Å—Ä–µ–∑–æ–≤")
    return all_slices


def train_autoencoder(slices: np.ndarray, epochs: int = 30, batch_size: int = 64, lr: float = 1e-3):
    """–û–±—É—á–∞–µ—Ç –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–æ–¥–µ–ª—å + –ª–æ–≥ –ø–æ—Ç–µ—Ä—å"""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    tensor_slices = torch.tensor(slices, dtype=torch.float32).unsqueeze(1)  # [N, 1, H, W]
    dataset = TensorDataset(tensor_slices)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # –ú–æ–¥–µ–ª—å
    model = Autoencoder2D().to(device)
    optimizer = optim.Adam(model.parameters(), lr=lr)
    criterion = nn.MSELoss()

    # –û–±—É—á–µ–Ω–∏–µ
    losses = []
    start_time = time.time()
    for epoch in range(epochs):
        model.train()
        epoch_loss = 0.0
        for batch, in dataloader:
            batch = batch.to(device)
            optimizer.zero_grad()
            recon = model(batch)
            loss = criterion(recon, batch)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()
        avg_loss = epoch_loss / len(dataloader)
        losses.append(avg_loss)
        logger.info(f"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.6f}")

    training_time_sec = time.time() - start_time
    logger.info("–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")
    return model, losses, training_time_sec, device


def evaluate_on_val_set(model, device, val_dir: Path = None):
    """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (normal + pathology), –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏."""
    from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix
    from backend.utils.excel_reporter import generate_excel_report

    if val_dir is None:
        val_dir = project_root / "backend" / "data" / "val"
        if not val_dir.exists():
            logger.warning("–ü–∞–ø–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –ü—Ä–æ–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞.")
            return None, None

    normal_dir = val_dir / "normal"
    patho_dir = val_dir / "pathology"

    results = []
    y_true = []
    y_pred = []

    for label, class_dir in [(0, normal_dir), (1, patho_dir)]:
        if not class_dir.exists():
            continue
        for zip_path in class_dir.glob("*.zip"):
            try:
                slices, study_uid, series_uid = load_slices_from_zip(str(zip_path))
                tensor_slices = torch.tensor(slices, dtype=torch.float32).unsqueeze(1).to(device)
                with torch.no_grad():
                    recon = model(tensor_slices)
                    mse = torch.mean((tensor_slices - recon) ** 2, dim=[1, 2, 3])
                    avg_mse = mse.mean().item()
                # –ü—Ä–æ—Å—Ç–∞—è —Å–∏–≥–º–æ–∏–¥–Ω–∞—è –∫–∞–ª–∏–±—Ä–æ–≤–∫–∞ (–∫–∞–∫ –≤ inference)
                prob = 1.0 / (1.0 + np.exp(-100 * (avg_mse - 0.001)))
                pred = 1 if prob > 0.5 else 0

                results.append({
                    "path_to_study": str(zip_path),
                    "study_uid": study_uid,
                    "series_uid": series_uid,
                    "probability_of_pathology": float(prob),
                    "pathology": int(pred),
                    "processing_status": "Success",
                    "time_of_processing": 0.0
                })
                y_true.append(label)
                y_pred.append(prob)
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {zip_path}: {e}")
                results.append({
                    "path_to_study": str(zip_path),
                    "study_uid": None,
                    "series_uid": None,
                    "probability_of_pathology": None,
                    "pathology": None,
                    "processing_status": "Failure",
                    "time_of_processing": 0.0
                })

    if not y_true:
        return None, results

    auc = roc_auc_score(y_true, y_pred)
    y_pred_bin = (np.array(y_pred) > 0.5).astype(int)
    acc = accuracy_score(y_true, y_pred_bin)
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred_bin, labels=[0, 1]).ravel()

    metrics = {
        "AUC": float(auc),
        "Accuracy": float(acc),
        "True_Negative": int(tn),
        "False_Positive": int(fp),
        "False_Negative": int(fn),
        "True_Positive": int(tp),
        "Threshold_used": 0.5
    }

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º Excel-–æ—Ç—á—ë—Ç
    generate_excel_report(results, str(VAL_REPORT_PATH))
    logger.info(f"–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π –æ—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {VAL_REPORT_PATH}")

    return metrics, results


def main():
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞ –Ω–∞ '–Ω–æ—Ä–º–µ'")

    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    slices = load_all_slices_from_normal(DATA_DIR)

    # 2. –û–±—É—á–µ–Ω–∏–µ
    model, losses, train_time, device = train_autoencoder(slices, epochs=30, batch_size=64)

    # 3. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    torch.save(model.state_dict(), MODEL_SAVE_PATH)
    logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {MODEL_SAVE_PATH}")

    # 4. –û—Ü–µ–Ω–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    metrics, _ = evaluate_on_val_set(model, device)

    # 5. –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á—ë—Ç–∞
    report = {
        "model_path": str(MODEL_SAVE_PATH.relative_to(project_root)),
        "training_time_seconds": train_time,
        "num_slices_trained": len(slices),
        "final_loss": losses[-1],
        "epochs": len(losses),
        "device": str(device),
        "metrics_on_validation": metrics or "Validation data not found"
    }

    with open(METRICS_REPORT_PATH, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=4, ensure_ascii=False)

    logger.info(f"üìä –û—Ç—á—ë—Ç –æ –º–µ—Ç—Ä–∏–∫–∞—Ö —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {METRICS_REPORT_PATH}")
    logger.info("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")


if __name__ == "__main__":
    main()
