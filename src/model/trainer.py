# src/model/trainer.py
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import os
import logging
import random
from pathlib import Path
from tqdm import tqdm
import pydicom  # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∑–¥–µ—Å—å, —Ç–∞–∫ –∫–∞–∫ –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤ __main__
from src.dataloader.dicom_loader import load_dicom_from_zip
from src.model.autoencoder_2d import UNet2D

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –°–æ–∑–¥–∞—ë–º –ø–∞–ø–∫—É models/, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
os.makedirs(os.path.dirname("models/autoencoder_2d.pth"), exist_ok=True)

# --- –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ï –ù–ê–°–¢–†–û–ô–ö–ò –î–õ–Ø –í–ê–®–ï–ô –°–ò–°–¢–ï–ú–´ ---
# –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: CUDA (NVIDIA GPU) –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞, –∏–Ω–∞—á–µ CPU
# –î–ª—è Windows 11 –∏ 3090 Ti —ç—Ç–æ –±—É–¥–µ—Ç 'cuda'
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {DEVICE}")

# –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
# BATCH_SIZE: –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏ 3090 Ti (24 –ì–ë)
# 64-128 - —Ö–æ—Ä–æ—à–∏–π –¥–∏–∞–ø–∞–∑–æ–Ω –¥–ª—è 2D U-Net –Ω–∞ 3090
BATCH_SIZE = 128

# NUM_EPOCHS: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö. 100-200 —Ç–∏–ø–∏—á–Ω–æ –¥–ª—è —Ö–æ—Ä–æ—à–µ–π —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏.
NUM_EPOCHS = 150

# LEARNING_RATE: –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –¥–ª—è Adam
LEARNING_RATE = 1e-3

# NUM_WORKERS: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–¥–ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö.
# –î–ª—è Windows —á–∞—Å—Ç–æ —Å—Ç–∞–±–∏–ª—å–Ω–µ–µ 0 –∏–ª–∏ 1, –Ω–æ 4 –º–æ–∂–µ—Ç —É—Å–∫–æ—Ä–∏—Ç—å, –µ—Å–ª–∏ HDD –±—ã—Å—Ç—Ä—ã–π.
# –ü–æ–ø—Ä–æ–±—É–µ–º 4, –µ—Å–ª–∏ –±—É–¥—É—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å DataLoader, —É–º–µ–Ω—å—à–∏—Ç—å –¥–æ 0.
NUM_WORKERS = 4

# PIN_MEMORY: –£—Å–∫–æ—Ä—è–µ—Ç –ø–µ—Ä–µ–Ω–æ—Å –¥–∞–Ω–Ω—ã—Ö CPU->GPU, –µ—Å–ª–∏ True.
# –û—á–µ–Ω—å –ø–æ–ª–µ–∑–Ω–æ –¥–ª—è NVIDIA GPU.
PIN_MEMORY = True if DEVICE.type == 'cuda' else False

# –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
SAVE_PATH = "models/autoencoder_2d.pth"


class CTNormalDataset(Dataset):
    """–ù–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, –∑–∞–≥—Ä—É–∂–∞—é—â–∏–π –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—â–∏–π –∫–∞–∂–¥—ã–π DICOM-—Å—Ä–µ–∑ –æ—Ç–¥–µ–ª—å–Ω–æ."""

    def __init__(self, zip_files):
        self.zip_files = zip_files
        self.all_slices = []  # –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö (zip_path, ds) –∫–æ—Ä—Ç–µ–∂–µ–π

        logger.info("–ù–∞—á–∞–ª–æ —Å–±–æ—Ä–∞ –≤—Å–µ—Ö DICOM-—Å—Ä–µ–∑–æ–≤ –∏–∑ ZIP-—Ñ–∞–π–ª–æ–≤...")
        for zip_path in self.zip_files:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–ø–∏—Å–æ–∫ pydicom.Dataset –∏–∑ –∫–∞–∂–¥–æ–≥–æ ZIP
            dicom_list = load_dicom_from_zip(str(zip_path))
            for ds in dicom_list:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –ø–∏–∫—Å–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                if hasattr(ds, 'pixel_array') and ds.pixel_array is not None:
                    # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä—Ç–µ–∂ (–ø—É—Ç—å –∫ ZIP, –æ–±—ä–µ–∫—Ç Dataset)
                    self.all_slices.append((str(zip_path), ds))
            logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω {zip_path.name}: –Ω–∞–π–¥–µ–Ω–æ {len(dicom_list)} DICOM-—Ñ–∞–π–ª–æ–≤.")

        logger.info(f"‚úÖ –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ {len(self.all_slices)} DICOM-—Å—Ä–µ–∑–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.")

    def __len__(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ä–µ–∑–æ–≤."""
        return len(self.all_slices)

    def __getitem__(self, idx):
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω DICOM-—Å—Ä–µ–∑ –ø–æ –∏–Ω–¥–µ–∫—Å—É.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (1, 128, 128) —Ç–∏–ø–∞ torch.float32.
        """
        try:
            # –ü–æ–ª—É—á–∞–µ–º –ø—É—Ç—å –∫ ZIP –∏ –æ–±—ä–µ–∫—Ç Dataset
            zip_path, ds = self.all_slices[idx]

            # 1. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–∏–∫—Å–µ–ª—å–Ω–æ–≥–æ –º–∞—Å—Å–∏–≤–∞
            pixel_array = ds.pixel_array.astype(np.float32)

            # 2. –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ HU (Hounsfield Units)
            slope = float(getattr(ds, 'RescaleSlope', 1.0))
            intercept = float(getattr(ds, 'RescaleIntercept', 0.0))
            hu_image = pixel_array * slope + intercept

            # 3. –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ HU (—Ç–∏–ø–∏—á–Ω–æ –¥–ª—è –ª–µ–≥–∫–∏—Ö)
            hu_image = np.clip(hu_image, -1000, 400)

            # 4. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω [0, 1]
            min_val = hu_image.min()
            max_val = hu_image.max()
            if max_val > min_val:
                hu_image_norm = (hu_image - min_val) / (max_val - min_val)
            else:
                hu_image_norm = np.zeros_like(hu_image)

            # 5. –†–µ—Å–µ–º–ø–ª–∏–Ω–≥ –¥–æ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ (128x128)
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º scipy.ndimage.zoom –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Ä–µ—Å–µ–º–ø–ª–∏–Ω–≥–∞
            from scipy.ndimage import zoom
            h, w = hu_image_norm.shape
            zoom_factors = (128 / h, 128 / w)
            # order=1 - –±–∏–ª–∏–Ω–µ–π–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è, —Ö–æ—Ä–æ—à–∞ –¥–ª—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            slice_resized = zoom(hu_image_norm, zoom_factors, order=1)

            # 6. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–∞–Ω–∞–ª–∞: (128, 128) -> (1, 128, 128)
            # DataLoader —Å–∞–º –æ–±—ä–µ–¥–∏–Ω–∏—Ç –≤ –±–∞—Ç—á (B, 1, 128, 128)
            tensor = torch.from_numpy(slice_resized).unsqueeze(0).float()

            # --- –ê–£–ì–ú–ï–ù–¢–ê–¶–ò–ò (–ø—Ä–æ—Å—Ç—ã–µ, –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏) ---
            # –°–ª—É—á–∞–π–Ω—ã–π —à—É–º
            if random.random() > 0.7:  # 30% –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å
                noise = torch.randn_like(tensor) * 0.02  # –ù–µ–±–æ–ª—å—à–æ–π —à—É–º
                tensor = tensor + noise
                tensor = torch.clamp(tensor, 0.0, 1.0)

            # –°–ª—É—á–∞–π–Ω—ã–π —Å–¥–≤–∏–≥ (–ø—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è)
            # if random.random() > 0.5:
            #     shift = random.randint(-5, 5)
            #     tensor = torch.roll(tensor, shifts=shift, dims=1)
            #     tensor = torch.roll(tensor, shifts=shift, dims=2)

            return tensor

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Ä–µ–∑–∞ idx={idx} –∏–∑ {getattr(ds, 'filename', 'Unknown')} –≤ {zip_path}: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω—É–ª–µ–≤–æ–π —Ç–µ–Ω–∑–æ—Ä –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏, DataLoader –º–æ–∂–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å
            return torch.zeros((1, 128, 128), dtype=torch.float32)


def train_autoencoder():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∞–≤—Ç–æ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞."""
    logger.info("========== –ó–ê–ü–£–°–ö –û–ë–£–ß–ï–ù–ò–Ø ==========")

    # 1. –ü–æ–∏—Å–∫ ZIP-—Ñ–∞–π–ª–æ–≤ —Å –æ–±—É—á–∞—é—â–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏
    train_data_path = Path("data/train_normal")
    zip_files = list(train_data_path.glob("*.zip"))
    logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(zip_files)} ZIP-—Ñ–∞–π–ª–æ–≤ –≤ {train_data_path}.")

    if len(zip_files) == 0:
        logger.error(f"‚ùó –í –ø–∞–ø–∫–µ {train_data_path} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ ZIP-—Ñ–∞–π–ª–æ–≤. –ü–æ–º–µ—Å—Ç–∏—Ç–µ —Ç—É–¥–∞ ZIP-–∞—Ä—Ö–∏–≤—ã —Å –Ω–æ—Ä–º–∞–ª—å–Ω—ã–º–∏ –ö–¢.")
        raise FileNotFoundError(f"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –≤ {train_data_path}")

    # 2. –°–æ–∑–¥–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö
    logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è CTNormalDataset...")
    dataset = CTNormalDataset(zip_files)

    if len(dataset) == 0:
        logger.error("‚ùó –ù–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –ø—É—Å—Ç. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ ZIP-—Ñ–∞–π–ª–æ–≤.")
        raise ValueError("–ù–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –ø—É—Å—Ç.")

    # 3. –°–æ–∑–¥–∞–Ω–∏–µ DataLoader
    # - batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
    # - shuffle: –ü–µ—Ä–µ–º–µ—à–∏–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–µ
    # - num_workers: –ü–æ–¥–ø—Ä–æ—Ü–µ—Å—Å—ã –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ (–º–æ–∂–µ—Ç —É—Å–∫–æ—Ä–∏—Ç—å –Ω–∞ –±—ã—Å—Ç—Ä—ã—Ö –¥–∏—Å–∫–∞—Ö)
    # - pin_memory: –£—Å–∫–æ—Ä—è–µ—Ç –ø–µ—Ä–µ–Ω–æ—Å –Ω–∞ GPU
    # - persistent_workers: (–¥–ª—è PyTorch >= 1.7) –ü–æ–≤—Ç–æ—Ä–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤–æ—Ä–∫–µ—Ä–æ–≤
    logger.info("–°–æ–∑–¥–∞–Ω–∏–µ DataLoader...")
    dataloader = DataLoader(
        dataset,
        batch_size=BATCH_SIZE,
        shuffle=True,
        num_workers=NUM_WORKERS,
        pin_memory=PIN_MEMORY,
        persistent_workers=True if NUM_WORKERS > 0 else False
    )
    logger.info(f"DataLoader —Å–æ–∑–¥–∞–Ω. Batch Size: {BATCH_SIZE}, Num Workers: {NUM_WORKERS}, Pin Memory: {PIN_MEMORY}")

    # 4. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏, —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
    logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ UNet2D...")
    # in_channels=1 (–≥—Ä–∞–¥–∞—Ü–∏–∏ —Å–µ—Ä–æ–≥–æ), out_channels=1 (–≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ)
    model = UNet2D(in_channels=1, out_channels=1, base_features=32).to(DEVICE)

    # MSE loss - —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –∞–≤—Ç–æ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–æ–≤
    criterion = nn.MSELoss()

    # Adam optimizer - —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω –∏ –∞–¥–∞–ø—Ç–∏–≤–µ–Ω
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

    logger.info(f"–ú–æ–¥–µ–ª—å, —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã. LR: {LEARNING_RATE}")

    # 5. –¶–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è
    logger.info(f"üöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ {DEVICE} –Ω–∞ {NUM_EPOCHS} —ç–ø–æ—Ö–∞—Ö...")
    for epoch in range(NUM_EPOCHS):
        model.train()  # –ü–µ—Ä–µ–≤–æ–¥–∏–º –º–æ–¥–µ–ª—å –≤ —Ä–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è
        total_loss = 0.0
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º tqdm –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        pbar = tqdm(dataloader, desc=f"Epoch {epoch + 1}/{NUM_EPOCHS}")

        for batch_idx, batch in enumerate(pbar):
            # batch - —Ç–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (B, 1, 128, 128)
            try:
                # 5.1. –ü–µ—Ä–µ–Ω–æ—Å–∏–º –±–∞—Ç—á –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (GPU/CPU)
                batch = batch.to(DEVICE, non_blocking=PIN_MEMORY)

                # 5.2. –û–±–Ω—É–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
                optimizer.zero_grad()

                # 5.3. –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ (–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ)
                recon_batch = model(batch)

                # 5.4. –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å
                loss = criterion(recon_batch, batch)  # MSE –º–µ–∂–¥—É –æ—Ä–∏–≥–∏–Ω–∞–ª–æ–º –∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ–º

                # 5.5. –û–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ö–æ–¥ (–≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤)
                loss.backward()

                # 5.6. –®–∞–≥ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ (–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤)
                optimizer.step()

                # 5.7. –ù–∞–∫–∞–ø–ª–∏–≤–∞–µ–º loss –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                total_loss += loss.item()

                # –û–±–Ω–æ–≤–ª—è–µ–º tqdm bar —Å —Ç–µ–∫—É—â–∏–º loss
                pbar.set_postfix({"loss": f"{loss.item():.6f}"})

            except RuntimeError as re:
                if "out of memory" in str(re).lower():
                    logger.error(f"‚ö†Ô∏è  CUDA out of memory –Ω–∞ –±–∞—Ç—á–µ {batch_idx}. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –±–∞—Ç—á.")
                    if hasattr(torch.cuda, 'empty_cache'):
                        torch.cuda.empty_cache()
                else:
                    logger.error(f"‚ö†Ô∏è  RuntimeError –Ω–∞ –±–∞—Ç—á–µ {batch_idx}: {re}")
                # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
                continue
            except Exception as e:
                logger.error(f"‚ö†Ô∏è  –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –Ω–∞ –±–∞—Ç—á–µ {batch_idx}: {e}")
                continue

        # 6. –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –∫–æ–Ω—Ü–µ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏
        avg_loss = total_loss / len(dataloader)
        logger.info(f"Epoch [{epoch + 1}/{NUM_EPOCHS}], –°—Ä–µ–¥–Ω–∏–π Loss: {avg_loss:.6f}")

        # 7. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏ (–º–æ–∂–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å, —Å–æ—Ö—Ä–∞–Ω—è—è –ª—É—á—à—É—é)
        try:
            torch.save(model.state_dict(), SAVE_PATH)
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {SAVE_PATH}")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏: {e}")

    logger.info("üéâ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    logger.info(f"–§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {SAVE_PATH}")


if __name__ == "__main__":
    train_autoencoder()
